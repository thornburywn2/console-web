# Console.web Log Aggregation Configuration
# Fluentd config for shipping logs to Elasticsearch/OpenSearch
#
# Usage:
# 1. Set LOG_FILE=/home/thornburywn/Projects/console-web/logs/app.log
# 2. Run Fluentd: fluentd -c /path/to/fluent.conf
# 3. Logs will be shipped to Elasticsearch

# =============================================================================
# SOURCES - Collect logs from application
# =============================================================================

# Application JSON logs (pino format)
<source>
  @type tail
  path /home/thornburywn/Projects/console-web/logs/*.log
  pos_file /var/log/fluentd/console-web.pos
  tag console-web.app
  read_from_head true
  
  <parse>
    @type json
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%LZ
    keep_time_key true
  </parse>
</source>

# PM2 logs (if using PM2)
<source>
  @type tail
  path /home/thornburywn/.pm2/logs/console-web-*.log
  pos_file /var/log/fluentd/console-web-pm2.pos
  tag console-web.pm2
  read_from_head false
  
  <parse>
    @type regexp
    expression /^(?<pm2_id>\d+)\|(?<process>[^\s]+)\s*\|\s*(?<message>.*)$/
  </parse>
</source>

# =============================================================================
# FILTERS - Transform and enrich logs
# =============================================================================

# Add hostname and environment tags
<filter console-web.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service console-web
    environment ${ENV.fetch('NODE_ENV', 'development')}
  </record>
</filter>

# Parse nested JSON in message field (for PM2 logs)
<filter console-web.pm2>
  @type parser
  key_name message
  reserve_data true
  remove_key_name_field false
  <parse>
    @type json
    @log_level warn
  </parse>
</filter>

# Map pino log levels to standard severity
<filter console-web.**>
  @type record_transformer
  enable_ruby true
  <record>
    severity ${record["level"] == 10 ? "TRACE" : record["level"] == 20 ? "DEBUG" : record["level"] == 30 ? "INFO" : record["level"] == 40 ? "WARN" : record["level"] == 50 ? "ERROR" : record["level"] == 60 ? "FATAL" : "UNKNOWN"}
  </record>
</filter>

# Mask sensitive data
<filter console-web.**>
  @type record_transformer
  enable_ruby true
  <record>
    # Mask authorization headers
    headers ${record["headers"]&.tap { |h| h["authorization"] = "[REDACTED]" if h["authorization"] }}
    # Mask passwords in body
    body ${record["body"]&.tap { |b| b["password"] = "[REDACTED]" if b["password"] }}
  </record>
</filter>

# =============================================================================
# OUTPUTS - Ship logs to destinations
# =============================================================================

# Elasticsearch/OpenSearch output
<match console-web.**>
  @type elasticsearch
  host ${ELASTICSEARCH_HOST}
  port ${ELASTICSEARCH_PORT}
  scheme https
  ssl_verify true
  user ${ELASTICSEARCH_USER}
  password ${ELASTICSEARCH_PASSWORD}
  
  # Index naming
  index_name console-web
  type_name _doc
  include_timestamp true
  
  # Use daily indices for easier retention management
  logstash_format true
  logstash_prefix console-web
  logstash_dateformat %Y.%m.%d
  
  # Buffer configuration
  <buffer>
    @type file
    path /var/log/fluentd/buffer/console-web
    flush_mode interval
    flush_interval 5s
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 60s
    retry_timeout 1h
    chunk_limit_size 8MB
    total_limit_size 512MB
    overflow_action drop_oldest_chunk
  </buffer>
  
  # Fallback to file on connection errors
  <secondary>
    @type file
    path /var/log/fluentd/fallback/console-web
    <buffer>
      @type file
      path /var/log/fluentd/fallback-buffer/console-web
    </buffer>
  </secondary>
</match>

# =============================================================================
# ALTERNATIVE: File output for testing
# =============================================================================
# Uncomment this section and comment out the Elasticsearch output above
# to test locally without Elasticsearch

# <match console-web.**>
#   @type file
#   path /var/log/fluentd/output/console-web
#   append true
#   <buffer time>
#     @type file
#     path /var/log/fluentd/file-buffer/console-web
#     timekey 1d
#     timekey_wait 10m
#     flush_mode interval
#     flush_interval 30s
#   </buffer>
#   <format>
#     @type json
#   </format>
# </match>

# =============================================================================
# ALTERNATIVE: Loki output for Grafana
# =============================================================================
# If using Grafana Loki instead of Elasticsearch

# <match console-web.**>
#   @type loki
#   url ${LOKI_URL}
#   
#   <label>
#     service console-web
#     environment ${record["environment"]}
#     severity ${record["severity"]}
#     component ${record["component"]}
#   </label>
#   
#   <buffer>
#     @type file
#     path /var/log/fluentd/loki-buffer/console-web
#     flush_interval 5s
#   </buffer>
# </match>
