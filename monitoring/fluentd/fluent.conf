# Console.web Log Aggregation Pipeline
# Fluentd configuration for collecting, transforming, and shipping logs

# =============================================================================
# SOURCES
# =============================================================================

# Tail JSON logs from application logs directory
<source>
  @type tail
  path /home/thornburywn/Projects/console-web/logs/*.log
  pos_file /var/log/fluentd/console-web.pos
  tag console-web.app
  read_from_head true

  <parse>
    @type json
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%LZ
    keep_time_key true
  </parse>
</source>

# Tail PM2 logs
<source>
  @type tail
  path /home/thornburywn/.pm2/logs/console-web-*.log
  pos_file /var/log/fluentd/pm2-console-web.pos
  tag console-web.pm2
  read_from_head true

  <parse>
    @type multiline
    format_firstline /^\d{4}-\d{2}-\d{2}/
    format1 /^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2} [+-]\d{4}): (?<message>.*)/
  </parse>
</source>

# Forward logs from Docker containers (if using containerized deployment)
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# =============================================================================
# FILTERS
# =============================================================================

# Add hostname and service tags to all logs
<filter console-web.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service console-web
    environment "#{ENV['NODE_ENV'] || 'development'}"
  </record>
</filter>

# Parse structured fields from message if present
<filter console-web.app>
  @type parser
  key_name message
  reserve_data true
  remove_key_name_field false
  emit_invalid_record_to_error false

  <parse>
    @type json
  </parse>
</filter>

# Mask sensitive data in logs
<filter console-web.**>
  @type record_transformer
  enable_ruby true
  <record>
    # Mask password fields
    message ${record["message"].to_s.gsub(/password['"]*\s*[:=]\s*['"][^'"]*['"]/, 'password="[MASKED]"')}
    # Mask API keys
    message ${record["message"].to_s.gsub(/api[_-]?key['"]*\s*[:=]\s*['"][^'"]*['"]/, 'apiKey="[MASKED]"')}
    # Mask authorization headers
    message ${record["message"].to_s.gsub(/authorization['"]*\s*[:=]\s*['"][^'"]*['"]/, 'authorization="[MASKED]"')}
    # Mask JWT tokens
    message ${record["message"].to_s.gsub(/eyJ[a-zA-Z0-9_-]*\.eyJ[a-zA-Z0-9_-]*\.[a-zA-Z0-9_-]*/, '[JWT_MASKED]')}
  </record>
</filter>

# Add log level normalization
<filter console-web.**>
  @type record_transformer
  enable_ruby true
  <record>
    log_level ${record["level"] || record["severity"] || "info"}
    # Normalize level names
    log_level_normalized ${
      level = (record["level"] || record["severity"] || "info").to_s.downcase
      case level
      when "10", "trace" then "trace"
      when "20", "debug" then "debug"
      when "30", "info" then "info"
      when "40", "warn", "warning" then "warn"
      when "50", "error" then "error"
      when "60", "fatal", "critical" then "fatal"
      else "info"
      end
    }
  </record>
</filter>

# =============================================================================
# OUTPUTS
# =============================================================================

# Primary output: Elasticsearch
<match console-web.**>
  @type elasticsearch
  host ${ELASTICSEARCH_HOST || 'localhost'}
  port ${ELASTICSEARCH_PORT || '9200'}
  user ${ELASTICSEARCH_USER}
  password ${ELASTICSEARCH_PASSWORD}

  # Index settings
  logstash_format true
  logstash_prefix console-web
  logstash_dateformat %Y.%m.%d

  # Buffer settings for reliability
  <buffer>
    @type file
    path /var/log/fluentd/buffer/elasticsearch
    flush_mode interval
    flush_interval 5s
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 60s
    retry_timeout 1h
    chunk_limit_size 8MB
    queue_limit_length 32
    overflow_action block
  </buffer>

  # Connection settings
  request_timeout 30s
  reload_connections true
  reconnect_on_error true
  reload_on_failure true
</match>

# Secondary output: Local file (for backup/debugging)
<match console-web.**>
  @type file
  path /var/log/fluentd/archive/console-web
  append true
  compress gzip

  <format>
    @type json
  </format>

  <buffer time>
    @type file
    path /var/log/fluentd/buffer/file
    timekey 1d
    timekey_wait 10m
    timekey_use_utc true
    flush_mode interval
    flush_interval 1h
  </buffer>
</match>

# Error output (for debugging Fluentd issues)
<label @ERROR>
  <match **>
    @type file
    path /var/log/fluentd/error
    <format>
      @type json
    </format>
  </match>
</label>

# =============================================================================
# ALTERNATIVE OUTPUTS (Uncomment as needed)
# =============================================================================

# # Loki output (for Grafana Loki)
# <match console-web.**>
#   @type loki
#   url ${LOKI_URL || 'http://localhost:3100'}
#   <label>
#     service console-web
#     environment ${ENV['NODE_ENV']}
#   </label>
#   <buffer>
#     @type file
#     path /var/log/fluentd/buffer/loki
#     flush_interval 5s
#   </buffer>
# </match>

# # CloudWatch Logs output (for AWS)
# <match console-web.**>
#   @type cloudwatch_logs
#   region ${AWS_REGION}
#   log_group_name /console-web/${ENV['NODE_ENV']}
#   log_stream_name ${hostname}
#   auto_create_stream true
#   <buffer>
#     @type file
#     path /var/log/fluentd/buffer/cloudwatch
#     flush_interval 5s
#   </buffer>
# </match>

# # Datadog output
# <match console-web.**>
#   @type datadog
#   api_key ${DATADOG_API_KEY}
#   service console-web
#   source nodejs
#   <buffer>
#     @type file
#     path /var/log/fluentd/buffer/datadog
#     flush_interval 5s
#   </buffer>
# </match>
